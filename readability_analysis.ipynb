{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Readability Analysis of Scientific Abstracts (2015-2025)\n",
        "\n",
        "This notebook analyzes readability trends in scientific literature using OpenAlex data.\n",
        "\n",
        "**Domains:** AI, Medicine, Business  \n",
        "**Metrics:** Flesch-Kincaid Grade Level, Automated Readability Index, Dale-Chall Score  \n",
        "**Sample size:** 1,000 abstracts per year per domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Install required Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install requests pandas textstat scipy matplotlib numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import csv\n",
        "import sys\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textstat\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration\n",
        "\n",
        "Set the domain to analyze. Change CONCEPT to one of:\n",
        "- \"AI\" (Artificial Intelligence - C154945302)\n",
        "- \"MEDICINE\" (Medicine - C71924100)\n",
        "- \"BUSINESS\" (Business - C144133560)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONCEPT = \"MEDICINE\"  # Change to \"AI\" or \"BUSINESS\" as needed\n",
        "\n",
        "# Map concepts to OpenAlex IDs\n",
        "CONCEPT_IDS = {\n",
        "    \"AI\": \"C154945302\",\n",
        "    \"MEDICINE\": \"C71924100\",\n",
        "    \"BUSINESS\": \"C144133560\"\n",
        "}\n",
        "\n",
        "# Output file names\n",
        "READABILITY_CSV_FILE = f'readability_scores_{CONCEPT}_2015_2025.csv'\n",
        "PLOT_FILENAME = f'readability_trends_{CONCEPT}_2015_2025.png'\n",
        "\n",
        "print(f\"Analyzing domain: {CONCEPT}\")\n",
        "print(f\"OpenAlex Concept ID: {CONCEPT_IDS[CONCEPT]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fetch Abstracts from OpenAlex\n",
        "\n",
        "Query OpenAlex API for abstracts published 2015-2025."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_openalex(year, concept_id):\n",
        "    \"\"\"\n",
        "    Fetch abstracts from OpenAlex for a specific year and concept.\n",
        "    Returns up to 1000 abstracts with inverted index format.\n",
        "    \"\"\"\n",
        "    BASE_URL = \"https://api.openalex.org/works\"\n",
        "    filters = {\n",
        "        \"filter\": f'concepts.id:{concept_id},publication_year:{year}',\n",
        "        \"per-page\": 200\n",
        "    }\n",
        "    \n",
        "    results = []\n",
        "    page = 1\n",
        "    \n",
        "    while True:\n",
        "        print(f\"Fetching OpenAlex page {page} for {year}...\")\n",
        "        resp = requests.get(BASE_URL, params={**filters, \"page\": page})\n",
        "        \n",
        "        if resp.status_code != 200:\n",
        "            print(f\"Error: API request failed with status code {resp.status_code}\")\n",
        "            print(f\"Response content: {resp.text}\")\n",
        "            break\n",
        "        \n",
        "        data = resp.json()\n",
        "        if \"results\" not in data or not data[\"results\"]:\n",
        "            break\n",
        "        \n",
        "        for work in data[\"results\"]:\n",
        "            if work.get(\"abstract_inverted_index\") is None:\n",
        "                continue\n",
        "            \n",
        "            results.append({\n",
        "                \"source\": \"OpenAlex\",\n",
        "                \"id\": work.get(\"id\"),\n",
        "                \"title\": work.get(\"title\"),\n",
        "                \"publication_year\": work.get(\"publication_year\"),\n",
        "                \"doi\": work.get(\"doi\"),\n",
        "                \"type\": work.get(\"type\"),\n",
        "                \"cited_by_count\": work.get(\"cited_by_count\"),\n",
        "                \"abstract\": work.get(\"abstract_inverted_index\"),\n",
        "                \"journal\": work.get(\"host_venue\", {}).get(\"display_name\"),\n",
        "                \"authors\": \", \".join([\n",
        "                    a.get(\"author\", {}).get(\"display_name\", \"\")\n",
        "                    for a in (work.get(\"authorships\") or [])\n",
        "                    if a and a.get(\"author\")\n",
        "                ])\n",
        "            })\n",
        "            \n",
        "            if len(results) >= 1000:\n",
        "                break\n",
        "        \n",
        "        if len(results) >= 1000:\n",
        "            break\n",
        "        \n",
        "        page += 1\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconstruct_abstract(inverted_index_str):\n",
        "    \"\"\"\n",
        "    Convert OpenAlex inverted index format to plain text.\n",
        "    Inverted index format: {\"word\": [position1, position2], ...}\n",
        "    \"\"\"\n",
        "    if pd.isna(inverted_index_str):\n",
        "        return \"\"\n",
        "    \n",
        "    try:\n",
        "        if isinstance(inverted_index_str, str):\n",
        "            inverted_index = ast.literal_eval(inverted_index_str)\n",
        "        else:\n",
        "            inverted_index = inverted_index_str\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: could not parse inverted index: {e}\")\n",
        "        return inverted_index_str\n",
        "    \n",
        "    if not isinstance(inverted_index, dict):\n",
        "        return str(inverted_index)\n",
        "    \n",
        "    word_positions = []\n",
        "    for word, positions in inverted_index.items():\n",
        "        try:\n",
        "            for pos in positions:\n",
        "                word_positions.append((pos, word))\n",
        "        except TypeError:\n",
        "            continue\n",
        "    \n",
        "    word_positions.sort(key=lambda x: x[0])\n",
        "    words_ordered = [word for pos, word in word_positions]\n",
        "    \n",
        "    return \" \".join(words_ordered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Download and Save Abstracts\n",
        "\n",
        "This will fetch abstracts for years 2015-2025 and save them as individual text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concept_id = CONCEPT_IDS[CONCEPT]\n",
        "\n",
        "for year in range(2015, 2026):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing year: {year}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    openalex_results = fetch_openalex(year, concept_id)\n",
        "    \n",
        "    df = pd.DataFrame(openalex_results)\n",
        "    df[\"abstract_text\"] = df[\"abstract\"].apply(reconstruct_abstract)\n",
        "    df.drop(columns=[\"abstract\"], inplace=True)\n",
        "    df = df[df[\"publication_year\"] == year]\n",
        "    \n",
        "    os.makedirs(f\"{CONCEPT}/abstracts_{year}\", exist_ok=True)\n",
        "    \n",
        "    saved_count = 0\n",
        "    for idx, row in df.iterrows():\n",
        "        abstract_text = row[\"abstract_text\"]\n",
        "        if pd.isna(abstract_text) or abstract_text == \"\":\n",
        "            continue\n",
        "        \n",
        "        filename = f\"{CONCEPT}/abstracts_{year}/abstract_{year}_{idx}.txt\"\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(abstract_text)\n",
        "        saved_count += 1\n",
        "    \n",
        "    print(f\"Saved {saved_count} abstracts to {CONCEPT}/abstracts_{year}/\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Data collection complete!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Calculate Readability Scores\n",
        "\n",
        "Define functions to calculate readability metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_corpus(directory_path, corpus_name):\n",
        "    \"\"\"\n",
        "    Scan directory for .txt files and calculate readability scores.\n",
        "    Returns list of dictionaries with scores for each abstract.\n",
        "    \"\"\"\n",
        "    corpus_results = []\n",
        "    \n",
        "    if not os.path.isdir(directory_path):\n",
        "        print(f\"Error: Directory not found: '{directory_path}'. Skipping.\")\n",
        "        return corpus_results\n",
        "    \n",
        "    print(f\"Processing corpus: '{corpus_name}' from directory: '{directory_path}'\")\n",
        "    \n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            paper_id = os.path.splitext(filename)[0]\n",
        "            \n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    text = f.read()\n",
        "                \n",
        "                if not text.strip():\n",
        "                    continue\n",
        "                \n",
        "                ari_score = textstat.automated_readability_index(text)\n",
        "                fkg_score = textstat.flesch_kincaid_grade(text)\n",
        "                dc_score = textstat.dale_chall_readability_score(text)\n",
        "                \n",
        "                corpus_results.append({\n",
        "                    'paper_id': paper_id,\n",
        "                    'corpus': corpus_name,\n",
        "                    'ari': ari_score,\n",
        "                    'flesch_kincaid_grade': fkg_score,\n",
        "                    'dale_chall_score': dc_score\n",
        "                })\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: Could not process {filename}. Error: {e}\")\n",
        "    \n",
        "    return corpus_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mean_sem(corpus_results):\n",
        "    \"\"\"\n",
        "    Calculate mean and standard error of the mean (SEM) for readability scores.\n",
        "    \"\"\"\n",
        "    ari_values = np.array([value[\"ari\"] for value in corpus_results])\n",
        "    flesch_values = np.array([value[\"flesch_kincaid_grade\"] for value in corpus_results])\n",
        "    dale_values = np.array([value[\"dale_chall_score\"] for value in corpus_results])\n",
        "    \n",
        "    if ari_values.size == 0:\n",
        "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 0\n",
        "    \n",
        "    n_samples = len(ari_values)\n",
        "    \n",
        "    ari_mean = ari_values.mean()\n",
        "    flesch_mean = flesch_values.mean()\n",
        "    dale_mean = dale_values.mean()\n",
        "    \n",
        "    ari_sem = stats.sem(ari_values)\n",
        "    flesch_sem = stats.sem(flesch_values)\n",
        "    dale_sem = stats.sem(dale_values)\n",
        "    \n",
        "    return ari_mean, ari_sem, flesch_mean, flesch_sem, dale_mean, dale_sem, n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_results_to_csv(all_results, output_file):\n",
        "    \"\"\"\n",
        "    Save results to CSV file.\n",
        "    \"\"\"\n",
        "    if not all_results:\n",
        "        print(\"No results to save.\")\n",
        "        return\n",
        "    \n",
        "    header = ['paper_id', 'corpus', 'ari', 'flesch_kincaid_grade', 'dale_chall_score']\n",
        "    \n",
        "    try:\n",
        "        with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=header)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(all_results)\n",
        "        print(f\"Successfully saved results to '{output_file}'\")\n",
        "    except IOError as e:\n",
        "        print(f\"ERROR: Could not write to CSV file. Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_readability_trends(years, ari_scores, ari_errors,\n",
        "                          flesch_scores, flesch_errors,\n",
        "                          dale_scores, dale_errors, concept):\n",
        "    \"\"\"\n",
        "    Generate line plot with error bars for readability scores.\n",
        "    \"\"\"\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    \n",
        "    plt.errorbar(years, ari_scores, yerr=ari_errors, marker='o', linestyle='-',\n",
        "                 label='Automated Readability Index (ARI)', capsize=5)\n",
        "    plt.errorbar(years, flesch_scores, yerr=flesch_errors, marker='s', linestyle='--',\n",
        "                 label='Flesch-Kincaid Grade', capsize=5)\n",
        "    plt.errorbar(years, dale_scores, yerr=dale_errors, marker='^', linestyle=':',\n",
        "                 label='Dale-Chall Score', capsize=5)\n",
        "    \n",
        "    plt.title(f'Readability Scores of {concept} Papers Abstracts (2015-2025) with SEM Error Bars',\n",
        "              fontsize=16)\n",
        "    plt.xlabel('Year', fontsize=12)\n",
        "    plt.ylabel('Readability Score', fontsize=12)\n",
        "    plt.xticks(years, rotation=45)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plot_filename = f'readability_trends_{concept}_2015_2025.png'\n",
        "    plt.savefig(plot_filename, dpi=300)\n",
        "    print(f\"Successfully saved plot to '{plot_filename}'\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Analysis\n",
        "\n",
        "Process all years and generate visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "years = list(range(2015, 2026))\n",
        "all_results = []\n",
        "\n",
        "ari_means = []\n",
        "flesch_means = []\n",
        "dale_means = []\n",
        "\n",
        "ari_sems = []\n",
        "flesch_sems = []\n",
        "dale_sems = []\n",
        "\n",
        "print(\"Starting readability analysis for years 2015-2025...\\n\")\n",
        "\n",
        "for year in years:\n",
        "    CORPUS_DIR = f'{CONCEPT}/abstracts_{year}'\n",
        "    corpus_results = process_corpus(CORPUS_DIR, str(year))\n",
        "    all_results.extend(corpus_results)\n",
        "    \n",
        "    ari_mean, ari_sem, flesch_mean, flesch_sem, dale_mean, dale_sem, n_samples = calculate_mean_sem(corpus_results)\n",
        "    \n",
        "    ari_means.append(ari_mean)\n",
        "    flesch_means.append(flesch_mean)\n",
        "    dale_means.append(dale_mean)\n",
        "    \n",
        "    ari_sems.append(ari_sem)\n",
        "    flesch_sems.append(flesch_sem)\n",
        "    dale_sems.append(dale_sem)\n",
        "    \n",
        "    print(f\"Completed {year} (n={n_samples}). \"\n",
        "          f\"ARI: {ari_mean:.2f} (\u00b1{ari_sem:.2f}), \"\n",
        "          f\"Flesch: {flesch_mean:.2f} (\u00b1{flesch_sem:.2f}), \"\n",
        "          f\"Dale: {dale_mean:.2f} (\u00b1{dale_sem:.2f})\")\n",
        "\n",
        "if all_results:\n",
        "    print(\"\\nSaving all raw results to CSV...\")\n",
        "    save_results_to_csv(all_results, READABILITY_CSV_FILE)\n",
        "else:\n",
        "    print(\"\\nNo .txt files found or processed.\")\n",
        "\n",
        "print(\"\\nGenerating plot with SEM error bars...\")\n",
        "plot_readability_trends(years, ari_means, ari_sems,\n",
        "                       flesch_means, flesch_sems,\n",
        "                       dale_means, dale_sems, CONCEPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Statistical Comparison (Optional)\n",
        "\n",
        "Compare two specific years using Welch's t-test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_years(year1, year2, concept):\n",
        "    \"\"\"\n",
        "    Compare readability scores between two years using Welch's t-test.\n",
        "    \"\"\"\n",
        "    dir1 = f'{concept}/abstracts_{year1}'\n",
        "    dir2 = f'{concept}/abstracts_{year2}'\n",
        "    \n",
        "    results1 = process_corpus(dir1, str(year1))\n",
        "    results2 = process_corpus(dir2, str(year2))\n",
        "    \n",
        "    ari1 = np.array([r[\"ari\"] for r in results1])\n",
        "    ari2 = np.array([r[\"ari\"] for r in results2])\n",
        "    \n",
        "    flesch1 = np.array([r[\"flesch_kincaid_grade\"] for r in results1])\n",
        "    flesch2 = np.array([r[\"flesch_kincaid_grade\"] for r in results2])\n",
        "    \n",
        "    dale1 = np.array([r[\"dale_chall_score\"] for r in results1])\n",
        "    dale2 = np.array([r[\"dale_chall_score\"] for r in results2])\n",
        "    \n",
        "    t_stat_ari, p_value_ari = stats.ttest_ind(ari1, ari2, equal_var=False)\n",
        "    t_stat_flesch, p_value_flesch = stats.ttest_ind(flesch1, flesch2, equal_var=False)\n",
        "    t_stat_dale, p_value_dale = stats.ttest_ind(dale1, dale2, equal_var=False)\n",
        "    \n",
        "    print(f\"\\nComparison: {year1} vs {year2}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"ARI:    {ari1.mean():.4f} vs {ari2.mean():.4f}, p = {p_value_ari:.6f}\")\n",
        "    print(f\"Flesch: {flesch1.mean():.4f} vs {flesch2.mean():.4f}, p = {p_value_flesch:.6f}\")\n",
        "    print(f\"Dale:   {dale1.mean():.4f} vs {dale2.mean():.4f}, p = {p_value_dale:.6f}\")\n",
        "    \n",
        "    alpha = 0.05\n",
        "    if p_value_ari < alpha and p_value_flesch < alpha and p_value_dale < alpha:\n",
        "        print(f\"\\nConclusion: Statistically significant difference (\u03b1 = {alpha})\")\n",
        "    else:\n",
        "        print(f\"\\nConclusion: No statistically significant difference (\u03b1 = {alpha})\")\n",
        "\n",
        "# Example: Compare 2018 (pre-AI) to 2024 (post-AI)\n",
        "compare_years(2018, 2024, CONCEPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary\n",
        "\n",
        "**Outputs Generated:**\n",
        "- CSV file with raw readability scores for all abstracts\n",
        "- PNG visualization showing readability trends with error bars\n",
        "- Statistical comparison between selected years\n",
        "\n",
        "**Next Steps:**\n",
        "- Repeat analysis for other domains by changing CONCEPT variable\n",
        "- Compare different year pairs using the compare_years function\n",
        "- Export results for further analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}